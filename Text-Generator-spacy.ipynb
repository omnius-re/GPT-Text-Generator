{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca26e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c23bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the spaCy model with pre-trained word embeddings ('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the text data\n",
    "with open('cote_v7.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c871684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stay\n",
      "up\n",
      "to\n",
      "date\n",
      "On\n",
      "Light\n",
      "Novels\n",
      "by\n",
      "Downloading\n",
      "our\n",
      "mobile\n",
      "App\n",
      "\n",
      "\n",
      "Zerobooks\n",
      "Android\n",
      "\n",
      "\n",
      "Zerobooks\n",
      "IOS\n",
      "\n",
      "\n",
      "Download\n",
      "all\n",
      "your\n",
      "Favorite\n",
      "Light\n",
      "Novels\n",
      "\n",
      "\n",
      "Jnovels.comTable\n",
      "of\n",
      "Contents\n",
      "\n",
      "\n",
      "Character\n",
      "Gallery\n",
      "\n",
      "\n",
      "Table\n",
      "of\n",
      "Contents\n",
      "Page\n",
      "\n",
      "\n",
      "Title\n",
      "Page\n",
      "\n",
      "\n",
      "Copyrights\n",
      "and\n",
      "Credits\n",
      "\n",
      "\n",
      "Chapter\n",
      "1\n",
      ":\n",
      "Ryuuen\n",
      "Kakeru\n",
      "â€™s\n",
      "Soliloquy\n",
      "\n",
      "\n",
      "Chapter\n",
      "2\n",
      ":\n",
      "The\n",
      "Sound\n",
      "of\n",
      "Footsteps\n",
      "in\n",
      "the\n",
      "Middle\n",
      "of\n",
      "Winter\n",
      "\n",
      "\n",
      "Chapter\n",
      "3\n",
      ":\n",
      "Reunions\n",
      "and\n",
      "Farewells\n",
      "\n",
      "\n",
      "Chapter\n",
      "4\n",
      ":\n",
      "Insanity\n",
      "\n",
      "\n",
      "Chapter\n",
      "5\n",
      ":\n",
      "Time\n",
      "to\n",
      "Settle\n",
      "Things\n",
      "\n",
      "\n",
      "Chapter\n",
      "6\n",
      ":\n",
      "Intersecting\n",
      "Thoughts\n",
      "\n",
      "\n",
      "Chapter\n",
      "7\n",
      ":\n",
      "What\n",
      "Ryuuen\n",
      "Wins\n",
      "and\n",
      "Loses\n"
     ]
    }
   ],
   "source": [
    "#Displays sample of tokenized document\n",
    "for token in doc[:100]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a07fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique words in the text\n",
    "words = sorted(list(set([token.text for token in doc])))\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d170a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n \\n', ' ', '!', '(', ')', ',', '-', '.', '1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180']\n",
      "5517\n"
     ]
    }
   ],
   "source": [
    "print(words[:100])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8acfb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from words to integers and vice versa\n",
    "stoi = {word: i for i, word in enumerate(words)}\n",
    "itos = {i: word for i, word in enumerate(words)}\n",
    "\n",
    "# Define a function to get word vectors from spaCy\n",
    "def get_word_vectors_batched(texts, batch_size=32):\n",
    "    # Tokenize the texts in batches\n",
    "    docs = list(nlp.pipe(texts, batch_size=batch_size)) # processes in batches\n",
    "    \n",
    "    # Find the maximum dimension of word vectors in the batch and avoid dimensional mismatch.\n",
    "    max_dim = max(max(token.vector.shape[0] for token in doc) for doc in docs)\n",
    "    \n",
    "    # Convert and pad word vectors to the maximum dimension for each token\n",
    "    padded_vectors = []\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            padded_vector = torch.cat((torch.tensor(token.vector), torch.zeros(max_dim - token.vector.shape[0])))\n",
    "            padded_vectors.append(padded_vector)\n",
    "    \n",
    "    return torch.stack(padded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d070861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "texts = [token.text for token in doc]\n",
    "data = get_word_vectors_batched(texts)\n",
    "n = int(0.9 * len(data))  # 90% of the data for training, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "450ff6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for loading data batches\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e5046bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for estimating loss (used for evaluation)\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f47d2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1c758cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    def __init__(self, head_size, input_features):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(input_features, head_size, bias=False)\n",
    "        self.query = nn.Linear(input_features, head_size, bias=False)\n",
    "        self.value = nn.Linear(input_features, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Unpacks batch size, sequence, length, input feature dimension\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "                \n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # Produces score matrix with stable gradients\n",
    "\n",
    "        # Calculate attention weights and probability values\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f4bc4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size, input_features, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, input_features) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(input_features, input_features)  # Adjusted input_features here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "90dff51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for feed-forward layers\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6ac94ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, dropout)  # Pass n_embd and dropout here\n",
    "        self.ffwd = FeedForward(n_embd)  # Assuming you have a FeedForward class defined\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  # LayerNorm after self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  # LayerNorm after feed-forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_sa = self.sa(x)  # Apply self-attention\n",
    "        x = x + self.ln1(x_sa)  # Add and normalize \n",
    "        x_ffwd = self.ffwd(x)  # Apply feed-forward\n",
    "        x = x + self.ln2(x_ffwd)  # Add and normalize\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1a97638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_layer=6, n_embd=96, n_head=1, vocab_size=5517, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # Check the batch size of x\n",
    "        batch_size_x = x.size(0)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Check the batch size of targets\n",
    "            batch_size_targets = targets.size(0)\n",
    "\n",
    "            # Rest of your code\n",
    "            B, T, C = x.shape\n",
    "            x = self.blocks(x)  # Pass through Transformer blocks\n",
    "            x = self.ln_f(x)  # Apply layer normalization\n",
    "            logits = self.lm_head(x)  # Generate logits for next token prediction\n",
    "\n",
    "            # Check if the batch size matches between logits and targets\n",
    "            if batch_size_x != batch_size_targets:\n",
    "                raise ValueError(\"Mismatched batch size between logits and targets\")\n",
    "\n",
    "            logits = logits.view(batch_size_x * T, -1)\n",
    "            targets = targets.view(batch_size_x * T)  # Flatten the targets to 1D\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1c6c26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.204653 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LanguageModel\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "186f7d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512]' is invalid for input of size 49152",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Every once in a while, evaluate the loss on the train and val sets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Sample a batch of training data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-myenv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 10\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      9\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 10\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     12\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp-myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[172], line 29\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched batch size between logits and targets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(batch_size_x \u001b[38;5;241m*\u001b[39m T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Flatten the targets to 1D\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[512]' is invalid for input of size 49152"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while, evaluate the loss on the train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of training data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss and perform backpropagation\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Generate text from the trained model\n",
    "context = torch.zeros((1, 1, n_embd), dtype=torch.float32, device=device)\n",
    "generated_text = []\n",
    "for _ in range(2000):\n",
    "    logits, _ = model(context)\n",
    "    predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "    generated_text.append(predicted_token.item())\n",
    "    context = torch.cat([context, model.blocks[-1](model.ln_f(context))[:, -1:, :]], dim=1)\n",
    "\n",
    "generated_text = [itos[i] for i in generated_text]\n",
    "generated_text = ' '.join(generated_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a9a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdd47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31382ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
